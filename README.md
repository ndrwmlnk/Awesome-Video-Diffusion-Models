## ðŸ“– Video Diffusion Models: A Survey

Survey link: https://openreview.net/forum?id=rJSHjhEYJx

```
@article{melnik2024video,
title={Video Diffusion Models: A Survey},
author={Andrew Melnik and Michal Ljubljanac and Cong Lu and Qi Yan and Weiming Ren and Helge Ritter},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=rJSHjhEYJx},
note={Survey Certification}
}
```

---

## Papers
[2024](#2024), [2023](#2023), [2022](#2022)
 
## 2024
 
[Lane Segmentation Refinement with Diffusion Models](https://arxiv.org/abs/2405.00620)  
 
[AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks](https://arxiv.org/abs/2403.14468)  
 
[VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models](https://arxiv.org/abs/2403.06098)  
 
[Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers](https://arxiv.org/abs/2402.19479)  
 
[EMO: Emote Portrait Alive-Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://arxiv.org/abs/2402.17485)  
 
[Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177)  
 
[Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)  
 
[Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis](https://arxiv.org/abs/2402.14797)  
 
[UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing](https://arxiv.org/abs/2402.13185)  
 
[ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation](https://arxiv.org/abs/2402.04324)  
 
[Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling](https://arxiv.org/abs/2401.15977)  
 
[Lumiere: A space-time diffusion model for video generation](https://arxiv.org/abs/2401.12945)  
 
[WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens](https://arxiv.org/abs/2401.09985)  
 
[Videocrafter2: Overcoming data limitations for high-quality video diffusion models](https://arxiv.org/abs/2401.09047)  
 
[Latte: Latent diffusion transformer for video generation](https://arxiv.org/abs/2401.03048)  
 
[Moonshot: Towards controllable video generation and editing with multimodal conditions](https://arxiv.org/abs/2401.01827)  
 
 
## 2023
 
[I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models](https://arxiv.org/abs/2312.16693)  
 
[Videopoet: A large language model for zero-shot video generation](https://arxiv.org/abs/2312.14125)  
 
[Llama guard: Llm-based input-output safeguard for human-ai conversations](https://arxiv.org/abs/2312.06674)  
 
[Photorealistic video generation with diffusion models](https://arxiv.org/abs/2312.06662)  
 
[GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation](https://arxiv.org/abs/2312.04557)  
 
[AnimateZero: Video Diffusion Models are Zero-Shot Image Animators](https://arxiv.org/abs/2312.03793)  
 
[Animate anyone: Consistent and controllable image-to-video synthesis for character animation](https://arxiv.org/abs/2311.17117)  
 
[Magicanimate: Temporally consistent human image animation using diffusion model](https://arxiv.org/abs/2311.16498)  
 
[Stable video diffusion: Scaling latent video diffusion models to large datasets](https://arxiv.org/abs/2311.15127)  
 
[Make pixels dance: High-dynamic video generation](https://arxiv.org/abs/2311.10982)  
 
[Emu video: Factorizing text-to-video generation by explicit image conditioning](https://arxiv.org/abs/2311.10709)  
 
[I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models](https://arxiv.org/abs/2311.04145)  
 
[Consistent Video-to-Video Transfer Using Synthetic Dataset](https://arxiv.org/abs/2311.00213)  
 
[Videocrafter1: Open diffusion models for high-quality video generation](https://arxiv.org/abs/2310.19512)  
 
[Dynamicrafter: Animating open-domain images with video diffusion priors](https://arxiv.org/abs/2310.12190)  
 
[FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing](https://arxiv.org/abs/2310.05922)  
 
[PixArt-$\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/abs/2310.00426)  
 
[Gaia-1: A generative world model for autonomous driving](https://arxiv.org/abs/2309.17080)  
 
[Show-1: Marrying pixel and latent diffusion models for text-to-video generation](https://arxiv.org/abs/2309.15818)  
 
[Lavie: High-quality video generation with cascaded latent diffusion models](https://arxiv.org/abs/2309.15103)  
 
[GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER](https://arxiv.org/abs/2309.13274)  
 
[Diffuse, attend, and segment: Unsupervised zero-shot segmentation using stable diffusion](https://arxiv.org/abs/2308.12469)  
 
[Tokenflow: Consistent diffusion features for consistent video editing](https://arxiv.org/abs/2307.10373)  
 
[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)  
 
[Sdxl: Improving latent diffusion models for high-resolution image synthesis](https://arxiv.org/abs/2307.01952)  
 
[Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation](https://arxiv.org/abs/2306.07954)  
 
[MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images](https://arxiv.org/abs/2306.07257)  
 
[Stable Remaster: Bridging the Gap Between Old Content and New Displays](https://arxiv.org/abs/2306.06803)  
 
[Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis](https://arxiv.org/abs/2306.03504)  
 
[Video Diffusion Models with Local-Global Context Guidance](https://arxiv.org/abs/2306.02562)  
 
[Probabilistic Adaptation of Text-to-Video Models](https://arxiv.org/abs/2306.01872)  
 
[Video Colorization with Pre-trained Text-to-Image Diffusion Models](https://arxiv.org/abs/2306.01732)  
 
[Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance](https://arxiv.org/abs/2306.00943)  
 
[Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising](https://arxiv.org/abs/2305.18264)  
 
[A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence](https://arxiv.org/abs/2305.15347)  
 
[VDT: An Empirical Study on Video Diffusion with Transformers](https://arxiv.org/abs/2305.13311)  
 
[ControlVideo: Training-free Controllable Text-to-Video Generation](https://arxiv.org/abs/2305.13077)  
 
[Any-to-Any Generation via Composable Diffusion](https://arxiv.org/abs/2305.11846)  
 
[VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](https://arxiv.org/abs/2305.10874)  
 
[Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation](https://arxiv.org/abs/2305.09662)  
 
[Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models](https://arxiv.org/abs/2305.08854)  
 
[Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts](https://arxiv.org/abs/2305.08850)  
 
[Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer](https://arxiv.org/abs/2305.05464)  
 
[AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion](https://arxiv.org/abs/2305.04001)  
 
[Motion-Conditioned Diffusion Model for Controllable Video Synthesis](https://arxiv.org/abs/2304.14404)  
 
[Generative Disco: Text-to-Video Generation for Music Visualization](https://arxiv.org/abs/2304.08551)  
 
[Text2Performer: Text-Driven Human Video Generation](https://arxiv.org/abs/2304.08483)  
 
[Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation](https://arxiv.org/abs/2304.08477)  
 
[Video Generation Beyond a Single Clip](https://arxiv.org/abs/2304.07483)  
 
[Dinov2: Learning robust visual features without supervision](https://arxiv.org/abs/2304.07193)  
 
[Soundini: Sound-Guided Diffusion for Natural Video Editing](https://arxiv.org/abs/2304.06818)  
 
[Segment anything](https://arxiv.org/abs/2304.02643)  
 
[Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos](https://arxiv.org/abs/2304.01186)  
 
[Zero-shot video editing using off-the-shelf image diffusion models](https://arxiv.org/abs/2303.17599)  
 
[Text2video-zero: Text-to-image diffusion models are zero-shot video generators](https://arxiv.org/abs/2303.13439)  
 
[Pix2video: Video editing using image diffusion](https://arxiv.org/abs/2303.12688)  
 
[NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](https://arxiv.org/abs/2303.12346)  
 
[Fatezero: Fusing attentions for zero-shot text-based video editing](https://arxiv.org/abs/2303.09535)  
 
[Decomposed Diffusion Models for High-Quality Video Generation](https://arxiv.org/abs/2303.08320)  
 
[Grounding dino: Marrying dino with grounded pre-training for open-set object detection](https://arxiv.org/abs/2303.05499)  
 
[Video-p2p: Video editing with cross-attention control](https://arxiv.org/abs/2303.04761)  
 
[Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971)  
 
[Adding conditional control to text-to-image diffusion models](https://arxiv.org/abs/2302.05543)  
 
[Structure and content-guided video synthesis with diffusion models](https://arxiv.org/abs/2302.03011)  
 
[Dreamix: Video diffusion models are general video editors](https://arxiv.org/abs/2302.01329)  
 
[Scenescape: Text-driven consistent scene generation](https://arxiv.org/abs/2302.01133)  
 
[Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models](https://arxiv.org/abs/2301.12597)  
 
[simple diffusion: End-to-end diffusion for high resolution images](https://arxiv.org/abs/2301.11093)  
 
[Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation](https://arxiv.org/abs/2301.03396)  
 
 
## 2022
 
[Behavioral cloning via search in video pretraining latent space](https://arxiv.org/abs/2212.13326)  
 
[Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://arxiv.org/abs/2212.11565)  
 
[Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)  
 
[Latent video diffusion models for high-fidelity video generation with arbitrary lengths](https://arxiv.org/abs/2211.13221)  
 
[Magicvideo: Efficient video generation with latent diffusion models](https://arxiv.org/abs/2211.11018)  
 
[Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models](https://arxiv.org/abs/2210.14896)  
 
[Representation Learning with Diffusion Models](https://arxiv.org/abs/2210.11058)  
 
[Imagen video: High definition video generation with diffusion models](https://arxiv.org/abs/2210.02303)  
 
[Make-a-video: Text-to-video generation without text-video data](https://arxiv.org/abs/2209.14792)  
 
[Prompt-to-prompt image editing with cross attention control](https://arxiv.org/abs/2208.01626)  
 
[An image is worth one word: Personalizing text-to-image generation using textual inversion](https://arxiv.org/abs/2208.01618)  
 
[Classifier-free diffusion guidance](https://arxiv.org/abs/2207.12598)  
 
[Cogvideo: Large-scale pretraining for text-to-video generation via transformers](https://arxiv.org/abs/2205.15868)  
 
[Flexible diffusion modeling of long videos](https://arxiv.org/abs/2205.11495)  
 
[Hierarchical text-conditional image generation with clip latents](https://arxiv.org/abs/2204.06125)  
 
[Video diffusion models](https://arxiv.org/abs/2204.03458)  
 
[Generating videos with dynamics-aware implicit generative adversarial networks](https://arxiv.org/abs/2202.10571)  
 
